{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  @USER en güzel uyuyan insan ödülü jeon jungkoo...      0\n",
      "1  @USER Mekanı cennet olsun, saygılar sayın avuk...      0\n",
      "2  Kızlar aranızda kas yığını beylere düşenler ol...      0\n",
      "3  Biraz ders çalışayım. Tembellik ve uyku düşman...      0\n",
      "4  @USER Trezeguet yerine El Sharawy daha iyi olm...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_excel('tr_Detecting Multilingual Offensive Language in Social Media.xlsx', names=['Text', 'Label'])\n",
    "# Print the first few rows of the resulting dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RAGHADBIRECILI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAGHADBIRECILI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RAGHADBIRECILI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "# Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop rows with missing values in the 'Text' column\n",
    "df['Text'].dropna(inplace=True)\n",
    "\n",
    "# Convert text to string data type\n",
    "df['Text'] = df['Text'].apply(str)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def normalize(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = normalize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem(text)\n",
    "    return text\n",
    "\n",
    "# Preprocess text\n",
    "df[\"Text\"] = df[\"Text\"].apply(preprocess)\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "from transformers import GPT2Config\n",
    "\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenize and encode text\n",
    "encoded_text = train_df[\"Text\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors and define dataloaders\n",
    "# Define maximum sequence length\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Pad sequences with zeros to ensure consistent length\n",
    "def pad_sequence(sequence):\n",
    "    if len(sequence) < MAX_LEN:\n",
    "        padding = [0] * (MAX_LEN - len(sequence))\n",
    "        sequence += padding\n",
    "    return sequence[:MAX_LEN]\n",
    "\n",
    "# Convert data to tensors and define dataloaders\n",
    "inputs = torch.tensor([pad_sequence(e) for e in encoded_text])\n",
    "labels = torch.tensor(train_df[\"Label\"].values)\n",
    "dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and encode text\n",
    "val_encoded_text = val_df[\"Text\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, padding=True))\n",
    "# Convert data to tensors and define dataloaders\n",
    "val_inputs = torch.tensor([pad_sequence(e) for e in val_encoded_text])\n",
    "val_labels = torch.tensor(val_df[\"Label\"].values)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs, val_labels)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 3128/3128 [23:07:03<00:00, 26.61s/batch, loss=0.486]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 0.4821086918025294 Validation Accuracy: 0.8209718670076727\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 3128/3128 [22:50:21<00:00, 26.29s/batch, loss=0.591]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Loss: 0.4041874845228765 Validation Accuracy: 0.8321611253196931\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 3128/3128 [21:38:37<00:00, 24.91s/batch, loss=0.107]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Loss: 0.36420912670609934 Validation Accuracy: 0.8308823529411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32 \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as pbar:\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # calculate validation accuracy\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "           for batch in val_dataloader:\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        val_acc = correct / total\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(\"Epoch:\", epoch + 1, \"Training Loss:\",avg_loss, \"Validation Accuracy:\", val_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to predict whether the comment is offensive or not\n",
    "def predict_offensiveness(model, tokenizer, comment):\n",
    "    model.eval()\n",
    "    encoded_comment = tokenizer.encode_plus(comment, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    input_ids = encoded_comment['input_ids'].to(device)\n",
    "    attention_mask = encoded_comment['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    _, predicted_class = torch.max(probabilities, dim=1)\n",
    "    return predicted_class.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment is offensive.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comment = \"Allah belanı versin iş yerinde açtım bütün ofis bana bakıyor\"\n",
    "predicted_class = predict_offensiveness(model, tokenizer, comment)\n",
    "if predicted_class == 0:\n",
    "    print(\"The comment is not offensive.\")\n",
    "else:\n",
    "    print(\"The comment is offensive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment is not offensive.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comment = \"Çok kötü inşallah kimseye bişey olmamıştır.\"\n",
    "predicted_class = predict_offensiveness(model, tokenizer, comment)\n",
    "if predicted_class == 0:\n",
    "    print(\"The comment is not offensive.\")\n",
    "else:\n",
    "    print(\"The comment is offensive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
